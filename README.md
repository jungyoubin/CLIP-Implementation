# CLIP Implementation

[[Blog]](https://openai.com/blog/clip/) [[Paper]](https://arxiv.org/abs/2103.00020) [[Colab]](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb)

CLIP (Contrastive Language-Image Pre-training) is a machine learning model developed by OpenAI that can understand and generate natural language descriptions of images. CLIP is trained on a diverse dataset of images paired with text descriptions, allowing it to learn a wide variety of visual concepts from natural language supervision.

## Approach

![CLIP](CLIP.png)

## Dataset
- [COCO 2017](https://cocodataset.org/#home)
